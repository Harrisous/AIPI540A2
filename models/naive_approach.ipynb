{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10942, 2)\n",
      "                                             content sentiment\n",
      "0  I love this app, but I do have one major gripe...  negative\n",
      "1  Trash. Yes, it has some nice nifty features bu...  negative\n",
      "2  OMG the UI is awful, seriously you have popup ...  negative\n",
      "3  I've been using the app for a while and since ...  negative\n",
      "4  Unable to register with an email. Clicking\"con...  negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.join('data','sentiment-analysis-dataset-google-play-app-reviews.csv'))\n",
    "df = df[['content','score']] # select content and score\n",
    "df.dropna()\n",
    "df['sentiment'] = df['score'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')\n",
    "df = df[['content','sentiment']]\n",
    "df = df[df['sentiment'] != 'neutral']  # Exclude neutral reviews\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords','punkt_tab'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clear_content(content):\n",
    "    '''this function will clear the text by following steps:'''\n",
    "    # step 1: expand contractions \n",
    "    content = contractions.fix(content) \n",
    "    # step 2: convert text to lower \n",
    "    content = content.lower()\n",
    "    # step 3: remove special characters\n",
    "    content = re.sub(r'[^a-zA-Z\\s]', '', content) \n",
    "    # step 4: tokenization\n",
    "    tokens = word_tokenize(content)\n",
    "    # step 5: lemmatization\n",
    "    cleared = []\n",
    "    for word in tokens:\n",
    "        if (word not in stop_words) and len(word) > 2: # exclude stop words and small words like a, an, it, as\n",
    "            cleared.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return ' '.join(cleared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "df['content'] = df['content'].apply(clear_content)\n",
    "\n",
    "# Convert text to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "# Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X, df['sentiment'])\n",
    "\n",
    "# new sentence prediction\n",
    "test_sentence = [\"The app is generally good but the price is too high\"]\n",
    "X_test = vectorizer.transform(test_sentence)\n",
    "prediction = model.predict(X_test)\n",
    "print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
